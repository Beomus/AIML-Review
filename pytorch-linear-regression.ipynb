{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables and offset by a constant (aka bias).\n",
    "\n",
    "_y = w * x + b_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_C_to_F(num):\n",
    "    return (num * 9/5) + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.4394255 ,  7.5692253 ,  3.072012  , 11.790111  ,  1.9905111 ,\n",
       "        0.90371615,  5.455385  , 12.137819  ,  2.0246139 ,  9.015684  ,\n",
       "       11.657337  ,  3.6755729 ,  3.372492  , 11.852789  ,  9.210675  ,\n",
       "        7.7850423 ,  6.697618  ,  1.2883344 ,  7.097615  ,  5.8117223 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array(np.random.uniform(low=0.5, high=13.3, size=(20,)), dtype='float32')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39.990967, 45.624607, 37.52962 , 53.2222  , 35.58292 , 33.62669 ,\n",
       "       41.819695, 53.848076, 35.644306, 48.228233, 52.983208, 38.61603 ,\n",
       "       38.070484, 53.335022, 48.579216, 46.013077, 44.055714, 34.319   ,\n",
       "       44.775707, 42.4611  ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.array(list(map(convert_C_to_F, inputs)), dtype='float32')\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting numpy arrays into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a linear regression model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0467]], requires_grad=True)\n",
      "tensor([0.3558], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.randn(1,  requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x * w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1483,  0.0020,  0.2122, -0.1952,  0.2627,  0.3135,  0.1008, -0.2115,\n",
       "          0.2611, -0.0656, -0.1890,  0.1840,  0.1982, -0.1981, -0.0747, -0.0081,\n",
       "          0.0428,  0.2955,  0.0241,  0.0842]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39.9910, 45.6246, 37.5296, 53.2222, 35.5829, 33.6267, 41.8197, 53.8481,\n",
       "        35.6443, 48.2282, 52.9832, 38.6160, 38.0705, 53.3350, 48.5792, 46.0131,\n",
       "        44.0557, 34.3190, 44.7757, 42.4611])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "Loss function is a way to evaluate how the model is performing, we can do that by comparing the model's predictions with the actual targets\n",
    "\n",
    "We can use Mean Squared Error (MSE):\n",
    "- Calculate the difference between `preds` and `targets`\n",
    "- Square all elements of the difference matrix\n",
    "- Calculate the average element in the difference matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "# .numel() returns the number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1925.1062, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MSE(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients\n",
    "\n",
    "with PyTorch, we can automatically compute gradien of the loss wrt to the weights and biases by using `requires_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0467]], requires_grad=True)\n",
      "tensor([[-599.0138]])\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a grad is **positive**:\n",
    "- increasing the weight will increase the loss\n",
    "- decreasing the weight will decrease the loss\n",
    "\n",
    "and vice versa, if a grad is **negative**:\n",
    "- increasing the weight will decrease the loss\n",
    "- decreasing the weight will increase the loss\n",
    "\n",
    "Check out more here [Wiki](https://en.wikipedia.org/wiki/Slope)\n",
    "\n",
    "The increase or ddecrease in the loss by changing a weight is proprotional to the gradient of the loss wrt to that element. This observation forms the basis of **gradient descend optimization** algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose the number `1e-5` (learning rate) to not modify the weights and biases significantly, taking a small step in the downhill direction.\n",
    "\n",
    "`torch.no_grad()` simply let PyTorch know not to track the gradient when we update the weights and biases manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_loss = MSE(preds, targets)\n",
    "updated_loss < loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to reset gradients by using `.zero-()` method because in PyTorch, gradients are calculated accumulatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "1. Generate predictions\n",
    "2. Calculate loss\n",
    "3. Compute gradients wrt to weights and biases\n",
    "4. Adjust weights and biases in proportion to gradient\n",
    "5. Reset gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1758,  0.0482,  0.2315, -0.1237,  0.2755,  0.3198,  0.1344, -0.1379,\n",
       "          0.2741, -0.0107, -0.1183,  0.2069,  0.2192, -0.1263, -0.0186,  0.0394,\n",
       "          0.0838,  0.3041,  0.0675,  0.1198]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1921.4449, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MSE(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-598.3618]])\n",
      "tensor([-86.6361])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * lr\n",
    "    b -= b.grad * lr\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1917.7914, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = MSE(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for _ in range(epochs):\n",
    "    preds = model(inputs)\n",
    "    loss = MSE(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * lr\n",
    "        b -= b.grad * lr\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1589.7255, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = MSE(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8015, 4.4661, 2.0742, 6.7110, 1.4990, 0.9210, 3.3418, 6.8960, 1.5172,\n",
       "         5.2354, 6.6404, 2.3952, 2.2340, 6.7444, 5.3391, 4.5809, 4.0025, 1.1256,\n",
       "         4.2153, 3.5314]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39.9910, 45.6246, 37.5296, 53.2222, 35.5829, 33.6267, 41.8197, 53.8481,\n",
       "        35.6443, 48.2282, 52.9832, 38.6160, 38.0705, 53.3350, 48.5792, 46.0131,\n",
       "        44.0557, 34.3190, 44.7757, 42.4611])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "epochs = 10000\n",
    "for _ in range(epochs):\n",
    "    preds = model(inputs)\n",
    "    loss = MSE(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * lr\n",
    "        b -= b.grad * lr\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39.9910, 45.6246, 37.5296, 53.2222, 35.5829, 33.6267, 41.8197, 53.8481,\n",
       "        35.6443, 48.2282, 52.9832, 38.6160, 38.0705, 53.3350, 48.5792, 46.0131,\n",
       "        44.0557, 34.3190, 44.7757, 42.4611])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[39.8817, 45.6018, 37.3826, 53.3159, 35.4061, 33.4198, 41.7385, 53.9513,\n",
       "         35.4684, 48.2453, 53.0732, 38.4857, 37.9318, 53.4304, 48.6017, 45.9962,\n",
       "         44.0088, 34.1228, 44.7398, 42.3897]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8276]], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([31.7683], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here you can see, our weight `w` and bias `b` are getting really close to the true value from the `convert_C_to_F` function we made in the beginning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
